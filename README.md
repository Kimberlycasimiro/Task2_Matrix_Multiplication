# Optimized Matrix Multiplication Approaches and Sparse Matrices

**Degree:** Data Science and Engineering  
**University:** University of Las Palmas de Gran Canaria  
**Course:** Big Data  
**Academic Year:** 2024 / 2025  
**Author:** Casimiro Torres, Kimberly

---

## Project Description

This project focuses on the development, optimization, and comparison of various matrix multiplication techniques for both dense and sparse matrices in high-performance environments. Matrix multiplication is an essential operation in numerous fields, including artificial intelligence, computer graphics, and scientific simulation. However, executing matrix multiplication on large matrices can be computationally costly, making it necessary to explore and apply optimization techniques to improve time and memory efficiency.

The main objective is to implement different matrix multiplication algorithms and analyze their performance under various configurations, evaluating factors such as matrix size and sparsity level. Optimizations include techniques like parallelization, cache blocking, loop unrolling, and advanced algorithms like Strassen and Winograd. Specific methods for sparse matrices in CSR and CSC formats have also been implemented to explore how sparsity affects computational efficiency.

---

## Project Structure

The project is organized in a modular structure to facilitate the understanding, execution, and analysis of each experiment. The folder and file structure is described below:

### `benchmark`
- **`benchmarkResults`**: Stores output files generated by benchmark tests. Contains three subdirectories:
    - **`combined`**: Results comparing dense and sparse matrices under varying sparsity levels.
        - `combined_benchmark_results.csv`: Execution time results for combined benchmarks.
        - `combined_memory_usage_results.csv`: Memory usage results for combined benchmarks.
    - **`dense`**: Results for dense matrix multiplication techniques.
        - `dense_benchmark_results.csv`: Execution time results for dense matrices.
        - `dense_memory_usage_results.csv`: Memory usage results for dense matrices.
    - **`sparse`**: Results for sparse matrix multiplication techniques.
        - `sparse_benchmark_results.csv`: Execution time results for sparse matrices.
        - `sparse_memory_usage_results.csv`: Memory usage results for sparse matrices.

- **`combined`**: Contains benchmarking code for evaluating both dense and sparse matrices with sparsity adjustments.
    - `MatrixMultiplicationBenchmark.java`: Benchmarks execution time for dense and sparse matrices with varying sparsity.
    - `MemoryUsageBenchmark.java`: Measures memory usage for dense and sparse matrices with varying levels of sparsity.

- **`dense`**: Benchmarking code specific to dense matrices.
    - `DenseMatrixMultiplicationBenchmark.java`: Benchmarks execution time for dense matrix multiplication methods.
    - `DenseMatrixMemoryUsage.java`: Measures memory usage for dense matrix operations.

- **`sparse`**: Benchmarking code specific to sparse matrices.
    - `SparseMatrixMultiplicationBenchmark.java`: Benchmarks execution time for sparse matrix multiplication methods.
    - `SparseMatrixMemoryUsage.java`: Measures memory usage for sparse matrix operations.

### `dense`
- **`algorithms`**:
    - `BasicMultiplication.java`: Standard dense matrix multiplication.
    - `StrassenAlgorithm.java`: Implements Strassen's recursive algorithm.
    - `WinogradAlgorithm.java`: Implements Winograd's optimized multiplication technique.
- **`optimizations`**:
    - `CacheBlockedMultiplication.java`: Uses cache blocking to improve performance.
    - `LoopUnrollingMultiplication.java`: Optimizes loops for better cache utilization.
    - `ParallelMultiplication.java`: Parallelized multiplication leveraging multi-core processing.
    - `VectorizedMultiplication.java`: Utilizes SIMD instructions for vectorized multiplication.

### `sparse`
- **`optimization`**:
    - `SparseMatrix.java`: Defines the structure of sparse matrices.
    - `SparseMultiplication.java`: Sparse matrix multiplication optimized for non-zero elements.
- **`structures`**:
    - `MatrixMultiplicationCSRCSC.java`: Manages combined sparse representations.
    - `SparseMatrixCSR.java`: Compressed Sparse Row (CSR) representation.
    - `SparseMatrixCSC.java`: Compressed Sparse Column (CSC) representation.
    - `Main.java`: Executes sparse matrix multiplication for `.mtx` matrices, measuring execution time and memory usage during CSR x CSC multiplication.

### `utils`
- **`utils`**: Utility classes that support configuration and data loading.
    - `ConfigLoader.java`: Loads configurations from `config.properties`.
    - `MatrixGenerator.java`: Generates matrices (dense and sparse) for testing.
    - `MatrixLoader.java`: Loads matrices from `.mtx` files for real-world testing scenarios.

### `resources`
- **`matrix_samples`**: Contains sample matrices in `.mtx` format.
    - `mc2depi.mtx`: Sample sparse matrix for performance testing.
    - `rajat23.mtx`: Another sample sparse matrix for comparative analysis.
- **`config.properties`**: Configuration file for defining matrix sizes, sparsity levels and thread count.

---

## Dependencies

This project relies on the following libraries and frameworks:

- **JMH (Java Microbenchmark Harness)**: Provides high-precision benchmarking capabilities, essential for accurately measuring execution time of matrix multiplication methods.
- **JUnit**: Used for testing and validation of individual methods to ensure correctness.
- **Maven**: Dependency management tool that facilitates the inclusion of libraries and handles project builds.

Ensure that all dependencies are defined in the `pom.xml` file, which allows Maven to automatically download and manage required libraries.

---

## Configuration

### `config.properties`

The `config.properties` file, located in the `resources` folder, allows users to configure important parameters without modifying the source code. Key parameters include:

- **Matrix Sizes**: Defines the sizes of matrices to be tested, ranging from small (64x64) to large (2048x2048).
- **Sparsity Levels**: Specifies the sparsity levels for sparse matrices, from 0.1 (10% non-zero) to 0.9 (90% sparsity).
- **Thread Count**: Configures the number of threads for parallel execution, useful for evaluating multi-core efficiency.

## Running the Project

### Building with Maven

First, build the project with Maven to ensure all dependencies are installed and the project is compiled:

```bash
mvn clean install
```

## Running Benchmarks

### Dense Matrix Benchmarks:

To run benchmarks for dense matrix multiplication techniques:

```bash
java -jar target/MatrixMultiplicationTask2-1.0-SNAPSHOT.jar org.ulpgc.benchmark.dense.DenseMatrixMultiplicationBenchmark
```

### Sparse Matrix Benchmarks:

To run benchmarks for sparse matrix multiplication techniques:

```bash
java -jar target/MatrixMultiplicationTask2-1.0-SNAPSHOT.jar org.ulpgc.benchmark.sparse.SparseMatrixMultiplicationBenchmark
```

### Combined Benchmarks (Dense and Sparse Comparison):

To run benchmarks comparing dense and sparse matrices with varying levels of sparsity:

```bash
java -jar target/MatrixMultiplicationTask2-1.0-SNAPSHOT.jar org.ulpgc.benchmark.combined.MatrixMultiplicationBenchmark
```

### Viewing Results

Results are stored in the `benchmarkResults` directory and are divided into three subdirectories (`dense`, `sparse`, `combined`), containing CSV files for both execution time and memory usage. These files provide a detailed breakdown of performance metrics for each technique.

---

## Implemented Algorithms and Methodology

Various matrix multiplication algorithms were implemented, each designed to maximize efficiency depending on matrix type and execution conditions:

- **Basic Matrix Multiplication**: Reference algorithm without optimizations.
- **Cache Blocked Matrix Multiplication**: Divides the matrix into blocks to improve cache locality.
- **Loop Unrolling Matrix Multiplication**: Reduces iteration overhead by unrolling loops.
- **Parallel Matrix Multiplication**: Divides the task among multiple cores, leveraging multicore hardware.
- **Strassen and Winograd Matrix Multiplication**: Advanced algorithms aiming to reduce the number of multiplications, with varying effectiveness depending on matrix size.
- **Sparse Matrix Multiplication**: Implementation of sparse matrices in CSR and CSC formats, focused on reducing memory usage and improving execution time in matrices where most elements are zero.

---

## Results and Analysis

### General Observations

The experiments reveal that there is no universally optimal matrix multiplication method; the best choice depends on matrix size, sparsity level, and the available hardware resources.

- **Memory Bottlenecks**: The Strassen method, while theoretically efficient, encounters memory bottlenecks for matrices larger than 1024 x 1024. Its recursive structure demands additional memory for temporary submatrices, leading to significant increases in both memory usage and execution time, making it less suitable for large matrices in constrained environments.
- **Maximum Efficient Matrix Size**: Across all techniques, the maximum efficiently handled matrix size was 2048 x 2048. Beyond this, resource limitations result in substantial increases in time and memory consumption, particularly for dense methods.
- **Dense vs. Sparse Performance**: Sparse matrices with high sparsity levels (0.8 and 0.9) show clear benefits in resource efficiency, significantly reducing execution time and memory usage compared to dense methods. This efficiency is maximized in applications where the majority of matrix elements are zero, making sparse methods ideal for these scenarios.

---

## Conclusion

This analysis shows that for dense matrices, optimized methods like **Cache Blocking**, **Parallelization**, and **Vectorization** deliver substantial performance improvements by reducing execution time while maintaining controlled memory usage. In particular, parallelization and vectorization are highly effective for large matrices, leveraging multicore processing and SIMD instructions to handle computations efficiently. In contrast, the **Strassen method** becomes less practical for large dense matrices due to its high memory demands, especially in resource-limited environments.

For sparse matrices, high sparsity levels allow sparse multiplication methods to excel, reducing both computation time and memory requirements. Sparse multiplication methods outperform dense techniques under these conditions, highlighting their suitability for highly sparse matrices.

Furthermore, experimenting with different sparsity levels on dense matrices provides additional insights into how sparsity can impact performance, even for algorithms typically optimized for dense data. Applying a sparsity level to dense matrices shows that, as the level of sparsity increases, the computational load decreases, which can lead to performance gains in certain optimized methods. This approach also reveals which techniques can adapt effectively to sparsity and leverage the structure of the data to optimize both memory usage and execution time.

Ultimately, this study underscores the importance of choosing a multiplication method that aligns with the matrix characteristics and system constraints. Dense matrices benefit from parallel and vectorized approaches, while highly sparse matrices achieve optimal performance with sparse-specific techniques. Each method has strengths tailored to particular scenarios, and understanding these nuances is essential for maximizing computational efficiency and effectively managing system resources.
